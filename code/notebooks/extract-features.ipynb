{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180f0612-65c0-4918-867a-b00dfe9ebe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a22e77b-ccaf-4f18-92ff-6a43b63e0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7556d4-fd6a-49c2-9165-760243d716af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81449ea5-478b-4616-9605-290e0da32c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231ab999-42af-476e-99b6-224c37b79a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8500c041-4a9a-4a1f-9ae7-699b9c66acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87b6586-6e6b-4261-8de2-aed9c226ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b3d6343f-81c2-40a1-9634-2743f47790cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/labeled-cleaned.csv', header=0)\n",
    "#del df['Unnamed: 0']\n",
    "#df['title_vectors_bert'] = df['title_vectors_bert'].apply(lambda x: ast.literal_eval(','.join(re.sub(r'(?<=\\d)(\\s+)(?=-?\\d)', ',', x).splitlines())))\n",
    "#df['article_sentence_vectors_bert'] = df['article_sentence_vectors_bert'].apply(lambda x: ast.literal_eval(re.sub(r'((array\\()|(dtype=float32\\),)|(dtype=float32\\)))', '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4fd7d48a-d451-4fea-b360-3553b934ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TL'] = df['title'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d017ba23-f94a-40d4-b2cf-43e194fdc89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AL'] = df['article'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1b0f2fad-3073-4ce6-9b40-4ed0be7564cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d712b21e-1b64-489e-9e30-8bbbbe307787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_tokens_spacy'] = df['title'].apply(lambda x: nlp(x))\n",
    "df['article_tokens_spacy'] = df['article'].apply(lambda x: nlp(x))\n",
    "df['title_lemmas_spacy'] = df.apply(lambda x: [token.lemma_ for token in x['title_tokens_spacy']], axis=1)\n",
    "df['article_lemmas_spacy'] = df.apply(lambda x: [token.lemma_ for token in x['article_tokens_spacy']], axis=1)\n",
    "df['title_tokens_no_stop_spacy'] = df['title'].apply(lambda x: nlp(' '.join([str(t) for t in nlp(x) if not t.is_stop])))\n",
    "df['article_tokens_no_stop_spacy'] = df['article'].apply(lambda x: nlp(' '.join([str(t) for t in nlp(x) if not t.is_stop])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3a14b719-e704-40e9-9b5d-e827f54620ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCount(text):\n",
    "    return len(TextBlob(text).words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b20857b2-7d09-48e0-af40-45d7fe65017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TWC'] = df['title'].apply(getWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "389f2bbb-4ba7-45fb-8376-3aa08903af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AWC'] = df['article'].apply(getWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7b6e110b-9903-417d-ac78-6e8b473bd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceCount(text):\n",
    "    return len(TextBlob(text).sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7b053236-7ef1-4b19-ae94-32d9ae2d9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TSC'] = df['title'].apply(getSentenceCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2d50d234-c915-4c56-84d2-0f75d1ffb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ASC'] = df['article'].apply(getSentenceCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c3574dcf-c274-4fc4-a625-1b920fa8bcbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['TALRATIO'] = df['TL'] / df['AL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3b52e6d1-c055-4efc-9c70-8116b9ad12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAWCRATIO'] = df['TWC'] / df['AWC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "eb62d702-f596-4f14-8c84-956b81e090dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TASCRATIO'] = df['TSC'] / df['ASC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "702c2b14-a197-459d-92cb-298e024cec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPolarity(text):\n",
    "    text_no_stop = ''\n",
    "    for token in text:\n",
    "        text_no_stop = text_no_stop + token.text\n",
    "    return TextBlob(text_no_stop).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4c145307-75bf-4438-b57a-cb80b1d79eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectivity(text):\n",
    "    text_no_stop = ''\n",
    "    for token in text:\n",
    "        text_no_stop = text_no_stop + token.text\n",
    "    return TextBlob(text_no_stop).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "509019db-02b1-451a-a96f-3dabac17c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPartitionPolarity(text, partition_number, partition_index):\n",
    "    start_index = len(text)//partition_number * (partition_index - 1)\n",
    "    end_index = len(text)//partition_number * partition_index\n",
    "\n",
    "    while start_index >= 0 and start_index < len(text) and text[start_index] != ' ':\n",
    "        start_index = start_index + 1\n",
    "    while end_index < len(text) and text[end_index] != ' ':\n",
    "        end_index = end_index + 1\n",
    "        \n",
    "    return TextBlob(text[start_index:end_index]).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a5751f52-f10f-4191-bffd-cb557418fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPartitionSubjectivity(text, partition_number, partition_index):\n",
    "    start_index = len(text)//partition_number * (partition_index - 1)\n",
    "    end_index = len(text)//partition_number * partition_index\n",
    "\n",
    "    while start_index >= 0 and start_index < len(text) and text[start_index] != ' ':\n",
    "        start_index = start_index + 1\n",
    "    while end_index < len(text) and text[end_index] != ' ':\n",
    "        end_index = end_index + 1\n",
    "        \n",
    "    return TextBlob(text[start_index:end_index]).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "28074d03-4bfa-40bb-a2a2-de7c55d7df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TP'] = df['title_tokens_no_stop_spacy'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "73bf0653-729a-41d2-80bc-94bdbbab178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AP'] = df['article_tokens_no_stop_spacy'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "207b5767-46cd-4c02-896f-8693ea37f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAPD'] = df['AP'] - df['TP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4f3a071b-4eb1-43f6-b719-4e6f93a116d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAPMEAN'] = (df['AP'] + df['TP']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "358da31c-f5f8-4f76-8ea7-d2334005d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TS'] = df['title_tokens_no_stop_spacy'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5ce292b7-2f1d-4340-a094-59b886de2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AS'] = df['article_tokens_no_stop_spacy'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "cbe1c989-f5f4-4075-92ca-ccb3966cdd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TASD'] = df['AS'] - df['TS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "471070bb-9efd-462f-b233-1ed71406d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TASMEAN'] = df['AS'] + df['TS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f7fe8dd0-ae81-466f-854e-04208cef9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/extracted_features_round1.csv')\n",
    "training_set = df.drop(columns=['index', 'title', 'article', 'title_tokens_spacy', 'article_tokens_spacy', 'title_lemmas_spacy', 'article_lemmas_spacy', 'title_tokens_no_stop_spacy', 'article_tokens_no_stop_spacy'])\n",
    "training_set = training_set.rename(columns={'label':'CV'})\n",
    "training_set.to_csv('../../data/extracted_features_round1_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4769c2f-f861-4667-8fa9-dab5ada09edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "62886be0-a781-48e0-8a03-02b897b9a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_special_character(string, divisor=1, from_start=True): \n",
    "    if from_start:\n",
    "        string = string[:-(len(string)//divisor)]\n",
    "    else:\n",
    "        string = string[len(string)//divisor:]\n",
    "    special_char= 0\n",
    "   \n",
    "    for i in range(0, len(string)):  \n",
    "      \n",
    "        ch = string[i]\n",
    "\n",
    "        if (string[i].isalpha()):  \n",
    "            continue\n",
    "\n",
    "        elif (string[i].isdigit()):\n",
    "            continue\n",
    "            \n",
    "        else: \n",
    "            special_char += 1\n",
    "            \n",
    "    return special_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "8e3c74fe-198a-4462-9aa7-632ec387f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TSCCRATIO'] = df['title'].apply(count_special_character, args=(4, False)) / df['TL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ac5fd5b6-688a-44e8-8152-799ed52fbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ASCCRATIO'] = df['article'].apply(count_special_character, args=(6,)) / df['AL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "699b5fbd-90d8-4e47-8040-c6216592fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TASCCRATIO'] = df['TSCCRATIO'] / df['ASCCRATIO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "72f05a79-4708-40b4-aee9-6c82400a4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TASCCD'] = df['ASCCRATIO'] - df['TSCCRATIO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50465e05-449d-4d37-b189-7c46de404a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a3145521-b662-41a3-97f0-2809fa0399ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "language = 'en'\n",
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fd964d37-a100-43eb-918b-1f141e3361b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_keywords'] = df['title'].apply(lambda x: custom_kw_extractor.extract_keywords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "0fde0cc6-ea67-42de-bef4-612a07d201c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_keywords'] = df['article'].apply(lambda x: custom_kw_extractor.extract_keywords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a89ca93e-cf17-4bdf-95fa-383e25c79aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TKWC'] = df['title_keywords'].apply(lambda x: len(x)) / df['TWC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c27d5cce-a1f6-47db-869b-f3d0b5603265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AKWC'] = df['article_keywords'].apply(lambda x: len(x)) / df['AWC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b0a2ad15-9926-4c98-b9c9-faea7bf7b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWCRATIO'] = df['TKWC'] / df['AKWC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fef6cdcd-2249-40c5-ac03-85455555bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_keyword_polarity(keywords):\n",
    "    if len(keywords) == 0:\n",
    "        return 0\n",
    "    \n",
    "    max_importance = min(number for word, number in keywords)\n",
    "    max_word = next((word for word, number in keywords if number == max_importance))\n",
    "    return TextBlob(max_word).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6d9124c5-dd6d-4877-bc20-5e450dfeee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TMIKWP'] = df['title_keywords'].apply(get_most_important_keyword_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "df5799c2-2528-4b84-bb0b-1528968abbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMIKWP'] = df['article_keywords'].apply(get_most_important_keyword_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ae36abb5-24d5-4da4-88f8-579b59ba4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_keyword_subjectivity(keywords):\n",
    "    if len(keywords) == 0:\n",
    "        return 0\n",
    "    \n",
    "    max_importance = min(number for word, number in keywords)\n",
    "    max_word = next((word for word, number in keywords if number == max_importance))\n",
    "    return TextBlob(max_word).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "59e15d8d-4ba4-4dbb-95b4-e71f4b10b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TMIKWS'] = df['title_keywords'].apply(get_most_important_keyword_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a6b41182-14fc-48c1-880c-cb6002d1a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMIKWS'] = df['article_keywords'].apply(get_most_important_keyword_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "19bfe5f8-080e-49d1-90c3-49ddb90f7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_sentence_polarity(text):\n",
    "    return TextBlob(text).sentences[0].sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "47a689a6-0566-479e-9421-a8872a1bfda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AFSP'] = df['article'].apply(get_first_sentence_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fb7ce21e-9af8-4413-8e9f-e04a59f57ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAFSPMEAN'] = (df['TP'] + df['AFSP']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ab568c65-5f2b-4d11-b45e-b2035d653fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAFSPD'] = df['AFSP'] - df['TP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6ce5a126-5864-472a-adfe-67d527d396bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_sentence_subjectivity(text):\n",
    "    return TextBlob(text).sentences[0].sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "efba898a-8449-4a95-82b7-188e4cc50974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AFSS'] = df['article'].apply(get_first_sentence_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "2bcaf980-b514-45bc-ad16-59ea7bda2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAFSSMEAN'] = (df['TS'] + df['AFSS']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d0921be0-959e-437e-b9c1-f80ffdb5ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAFSSD'] = df['AFSS'] - df['TP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "db33fa54-9b3a-4a46-84cf-e12772525696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_keyword_occurrence(row):\n",
    "    title_keywords = row['title_keywords']\n",
    "    article_tokens = row['article_tokens_spacy']\n",
    "    if len(title_keywords) == 0 or len(article_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    title_max_importance = min(number for word, number in title_keywords)\n",
    "    title_max_word = next((word for word, number in title_keywords if number == title_max_importance))\n",
    "    title_max_lemma = nlp(title_max_word)[0].lemma_\n",
    "    \n",
    "    article_lemmas = row['article_lemmas_spacy']\n",
    "    \n",
    "    occurrences = 0\n",
    "\n",
    "    for lemma in article_lemmas:\n",
    "        if lemma.lower() == title_max_lemma.lower():\n",
    "            occurrences = occurrences + 1\n",
    "    return occurrences / len(article_lemmas)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "6e94ad86-0edd-4234-89b7-af4121be6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TMIKWAORATIO'] = df.apply(get_most_important_keyword_occurrence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "0daee85b-5f14-4750-bb15-9d2dd9c234fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_keyword_lemmas_spacy'] = df['title_keywords'].apply(lambda x: [(nlp(keyword)[0].lemma_, number) for keyword, number in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "2845e829-404d-4762-937c-3126dee5451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_keyword_occurrence(row):\n",
    "    title_keywords = row['title_keywords']\n",
    "    article_tokens = row['article_tokens_spacy']\n",
    "    if len(title_keywords) == 0 or len(article_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    \n",
    "    article_lemmas = row['article_lemmas_spacy']\n",
    "    \n",
    "    occurrences = 0\n",
    "    for title_lemma, title_number in title_lemmas:\n",
    "        for article_lemma in article_lemmas:\n",
    "            if article_lemma.lower() == title_lemma.lower():\n",
    "                occurrences = occurrences + 1\n",
    "    return occurrences / len(article_lemmas)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "6348b621-e237-47d1-b452-e2e6e5b556af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TKWAORATIO'] = df.apply(get_title_keyword_occurrence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "7e05a4e3-f41a-4540-b26a-3ac7c326a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['TKWAORATIO'].to_numpy()\n",
    "min_nonzero = numpy.min(data[numpy.nonzero(data)])\n",
    "min_nonzero\n",
    "df['TKWAORATIO'] = df['TKWAORATIO'].apply(lambda x: x if x > 0 else min_nonzero)\n",
    "df['TKWAORATIO'] = df['TKWAORATIO'].apply(lambda x: numpy.where(x > 0.0000000001, numpy.log10(x), -10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5d5e9ec0-8ab9-4ba9-9c0f-4d354ec82123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_keyword_occurrence_weighted(row):\n",
    "    title_keywords = row['title_keywords']\n",
    "    article_tokens = row['article_tokens_spacy']\n",
    "    if len(title_keywords) == 0 or len(article_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    \n",
    "    article_lemmas = row['article_lemmas_spacy']\n",
    "    \n",
    "    occurrences = 0\n",
    "    for title_lemma, title_number in title_lemmas:\n",
    "        for article_lemma in article_lemmas:\n",
    "            if article_lemma.lower() == title_lemma.lower():\n",
    "                occurrences = occurrences + (1 * title_number)\n",
    "    return occurrences / len(article_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5243503d-d8f1-4480-a0b3-4bde616ee6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TKWAOWRATIO'] = df.apply(get_title_keyword_occurrence_weighted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2a8928de-17d0-4913-8117-bfed46446428",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['TKWAOWRATIO'].to_numpy()\n",
    "min_nonzero = numpy.min(data[numpy.nonzero(data)])\n",
    "min_nonzero\n",
    "df['TKWAOWRATIO'] = df['TKWAOWRATIO'].apply(lambda x: x if x > 0 else min_nonzero)\n",
    "df['TKWAOWRATIO'] = df['TKWAOWRATIO'].apply(lambda x: numpy.where(x > 0.0000000001, numpy.log10(x), -10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b0311e0f-f66e-442e-b1f0-72400cf31de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_keyword_polarity(keywords):\n",
    "    count = 0\n",
    "    polarity = 0\n",
    "    for word, number in keywords:\n",
    "        polarity = polarity + TextBlob(word).sentiment.polarity\n",
    "        count = count + 1\n",
    "        \n",
    "    if count != 0:\n",
    "        return polarity / count\n",
    "    \n",
    "    return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6ea2a839-cb2d-43ec-b1c3-c67113b6b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TKWPMEAN'] = df['title_keywords'].apply(get_mean_keyword_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "718e56b1-d27a-49ed-8f6d-463628f63a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AKWPMEAN'] = df['article_keywords'].apply(get_mean_keyword_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "87f86fa8-028f-402a-b583-769059b8153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWPMEAN'] = (df['TKWPMEAN'] + df['AKWPMEAN']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "77cc6765-2267-40cc-ab5e-3583884d4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWPD'] = df['AKWPMEAN'] - df['TKWPMEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "24029c9f-8057-4245-b61e-78bcd8b918dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_keyword_polarity_weighted(keywords):\n",
    "    count = 0\n",
    "    polarity = 0\n",
    "    for word, number in keywords:\n",
    "        polarity = polarity + (TextBlob(word).sentiment.polarity * number)\n",
    "        count = count + 1\n",
    "        \n",
    "    if count != 0:\n",
    "        return polarity / count\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "3695e032-0424-4c45-af5d-82188f0c9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TKWPWMEAN'] = df['title_keywords'].apply(get_mean_keyword_polarity_weighted)\n",
    "df['AKWPWMEAN'] = df['article_keywords'].apply(get_mean_keyword_polarity_weighted)\n",
    "df['TAKWPWMEAN'] = (df['TKWPWMEAN'] + df['AKWPWMEAN']) / 2\n",
    "df['TAKWPWD'] = df['AKWPWMEAN'] - df['TKWPWMEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "7bfbc808-7588-410d-a04c-9e47963fa80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_keyword_first_position(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    article_lemmas = row['article_lemmas_spacy']\n",
    "    if len(title_lemmas) == 0 or len(article_lemmas) == 0:\n",
    "        return 1\n",
    "    \n",
    "    title_max_importance = min(number for word, number in title_lemmas)\n",
    "    title_max_lemma = next((word for word, number in title_lemmas if number == title_max_importance))\n",
    "    \n",
    "    position = 0\n",
    "    for lemma in article_lemmas:\n",
    "        position = position + 1\n",
    "        if lemma.lower() == title_max_lemma.lower():\n",
    "            break\n",
    "            \n",
    "    return position / len(article_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a8de4068-f0ea-4b91-932f-29facf9142c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMIKWFP'] = df.apply(get_most_important_keyword_first_position, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "d1fdd1af-8803-49ee-b932-f5c2ac9cea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_keyword_distribution(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    article_lemmas = row['article_lemmas_spacy']\n",
    "    if len(title_lemmas) == 0 or len(article_lemmas) == 0:\n",
    "        return 0\n",
    "    \n",
    "    title_max_importance = min(number for word, number in title_lemmas)\n",
    "    title_max_lemma = next((word for word, number in title_lemmas if number == title_max_importance))\n",
    "    \n",
    "    position = 0\n",
    "    occurrences = []\n",
    "    for lemma in article_lemmas:\n",
    "        if lemma.lower() == title_max_lemma.lower():\n",
    "            occurrences.append(position)\n",
    "        position = position + 1\n",
    "    \n",
    "    if len(occurrences) == 0:\n",
    "        return 0\n",
    "    \n",
    "    distribution = 0\n",
    "    for position in occurrences:\n",
    "        distribution = distribution + (position - (len(article_lemmas) - 1) / 2)\n",
    "    \n",
    "    distribution = distribution / len(occurrences)\n",
    "            \n",
    "    return distribution / ((len(article_lemmas) - 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3d267929-befd-47f9-8b84-280967e90bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMIKWDIST'] = df.apply(get_most_important_keyword_distribution, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "83018e27-c0ca-45f0-ba58-95f2a76082da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_distribution(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    article_lemmas = row['article_lemmas_spacy']\n",
    "    if len(title_lemmas) == 0 or len(article_lemmas) == 0:\n",
    "        return 0\n",
    "    \n",
    "    occurrences = []\n",
    "    for title_lemma, title_number in title_lemmas:\n",
    "        position = 0\n",
    "        for article_lemma in article_lemmas:\n",
    "            if article_lemma.lower() == title_lemma.lower():\n",
    "                occurrences.append(position)\n",
    "            position = position + 1\n",
    "    \n",
    "    if len(occurrences) == 0:\n",
    "        return 0\n",
    "    \n",
    "    distribution = 0\n",
    "    for position in occurrences:\n",
    "        distribution = distribution + (position - (len(article_lemmas) - 1) / 2)\n",
    "    \n",
    "    distribution = distribution / len(occurrences)\n",
    "            \n",
    "    return distribution / ((len(article_lemmas) - 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a41e75e1-6a06-43bf-bb33-e7f7fbf50aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWDIST'] = df.apply(get_keyword_distribution, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "2325a7fc-2e87-4241-9c22-e680a5218932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/extracted_features_round2.csv')\n",
    "training_set = df.drop(columns=['index', 'title', 'article', 'title_keywords', 'article_keywords', 'title_tokens_spacy', 'article_tokens_spacy', 'title_lemmas_spacy', 'article_lemmas_spacy', 'title_keywords', 'article_keywords', 'title_keyword_lemmas_spacy', 'title_tokens_no_stop_spacy', 'article_tokens_no_stop_spacy'])\n",
    "training_set = training_set.rename(columns={'label':'CV'})\n",
    "training_set.to_csv('../../data/extracted_features_round2_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622adba8-5c66-4358-b548-5c0b09d5ac2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "ecb755e3-8290-42e8-a0c7-4d026f10a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_doc = row ['article_tokens_no_stop_spacy']\n",
    "    \n",
    "    return title_doc.similarity(article_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "44e4b9a3-dcdd-40e7-b78f-d9ea87b4b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TASIMSPACY'] = df.apply(get_similarity_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c42acd60-aeaf-4e88-9f5b-ff9ba7c82b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_sentences'] = df['article'].apply(lambda x: TextBlob(x).sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "8c67bfa7-b76e-4cce-b231-156264c743ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_sentence_tokens_spacy'] = df['article_sentences'].apply(lambda x: [nlp(str(sentence)) for sentence in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1569ab4a-6fee-47f1-99d4-c82ba050508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_sentence_tokens_no_stop_spacy'] = df['article_sentence_tokens_spacy'].apply(lambda x: [nlp(' '.join([str(t) for t in sentence if not t.is_stop])) for sentence in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "15e5a71a-88a3-48b7-8eb1-08a22cd139cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_sentence_similarity_score(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "        \n",
    "    scores = []\n",
    "    for doc in article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "    \n",
    "    return max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "16bb7d21-890e-420c-8d97-301cdb2f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp/ipykernel_4592/4069332995.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  scores.append(title_doc.similarity(doc))\n"
     ]
    }
   ],
   "source": [
    "df['TABSSIMSPACY'] = df.apply(get_best_sentence_similarity_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "b60368f0-82b3-4ad8-9ef8-4ecd69cd1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_sentence_similarity_score(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "        \n",
    "    scores = []\n",
    "    for doc in article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "    \n",
    "    return min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8135a437-d679-41ad-8a56-4b15ffc5b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp/ipykernel_4592/2013556861.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  scores.append(title_doc.similarity(doc))\n"
     ]
    }
   ],
   "source": [
    "df['TAWSSIMSPACY'] = df.apply(get_worst_sentence_similarity_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "221c996d-27b9-4253-8833-9c81c7b3773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_sentence_similarity_score(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "        \n",
    "    score = 0\n",
    "    for doc in article_docs:\n",
    "        score = score + (title_doc.similarity(doc))\n",
    "    \n",
    "    return score / len(article_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d3de6060-eb73-4eb6-a0b1-0251948345d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp/ipykernel_4592/2634829990.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score = score + (title_doc.similarity(doc))\n"
     ]
    }
   ],
   "source": [
    "df['TAASSIMSPACY'] = df.apply(get_average_sentence_similarity_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "331b1334-4753-4527-95f1-3ba53b42de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_sentence_similarity_score(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_doc = row['article_sentence_tokens_no_stop_spacy'][0]\n",
    "    \n",
    "    return title_doc.similarity(article_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "f38f5595-11e8-474d-b871-b595a4cfa90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAFSSIMSPACY'] = df.apply(get_first_sentence_similarity_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "22001c77-ded9-4016-af2a-5316f5368c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_sentence_similarity_position(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "        \n",
    "    scores = []\n",
    "    for doc in article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "    \n",
    "    max_score = max(scores)\n",
    "    \n",
    "    return (scores.index(max_score) + 1) / len(article_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "03958cfb-00c6-4c6e-a9dc-84b369a55be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp/ipykernel_4592/2584009954.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  scores.append(title_doc.similarity(doc))\n"
     ]
    }
   ],
   "source": [
    "df['TABSSIMPSPACY'] = df.apply(get_best_sentence_similarity_position, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "22254d66-9fd1-4e40-aca9-361d421a6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_sentence_similarity_position(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "        \n",
    "    scores = []\n",
    "    for doc in article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "    \n",
    "    min_score = min(scores)\n",
    "    \n",
    "    return (scores.index(min_score) + 1) / len(article_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "8269b981-152e-4191-9494-09b6b0318f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp/ipykernel_4592/3901347589.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  scores.append(title_doc.similarity(doc))\n"
     ]
    }
   ],
   "source": [
    "df['TAWSSIMPSPACY'] = df.apply(get_worst_sentence_similarity_position, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a917e423-17a6-4273-bd1d-0d9c4916e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_sentence_similarity_ratio(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "        \n",
    "    scores = []\n",
    "    for doc in article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "        \n",
    "    condition = lambda x: x>=0.90\n",
    "    \n",
    "    return sum(condition(x) for x in scores) / len(article_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "76d62ebb-b215-4df7-8fb2-1d2d318a6d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp/ipykernel_4592/3374778989.py:7: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  scores.append(title_doc.similarity(doc))\n"
     ]
    }
   ],
   "source": [
    "df['TAGSSIMRATIOSPACY'] = df.apply(get_good_sentence_similarity_ratio, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1c01d1b9-be28-4a33-84a8-80b9da9cb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_types(doc, word_type):\n",
    "    count = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == word_type:\n",
    "            count = count + 1\n",
    "    \n",
    "    return count / len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "33a88e4b-71f1-46c5-a72f-5ffa43637c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TVC'] = df['title_tokens_spacy'].apply(count_word_types, args=('VERB',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "40fcb13d-86ae-4630-a407-faa9683b3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVC'] = df['article_tokens_spacy'].apply(count_word_types, args=('VERB',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "c26a0cd4-c75e-4d05-823e-c2ac01d76b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAVCD'] = df['AVC'] - df['TVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd47918-c851-4203-a146-9b2257f157a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "36fa0e02-0454-40f0-9cb9-7c8ca93f2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TNC'] = df['title_tokens_spacy'].apply(count_word_types, args=('NOUN',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "7ec01ebd-88d4-4fa9-9e8d-36e2ec65a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ANC'] = df['article_tokens_spacy'].apply(count_word_types, args=('NOUN',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "935d5fd2-3c0a-4f45-a88f-83b0fbc2e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TANCD'] = df['ANC'] - df['TNC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c5c40b53-c953-4ef9-a325-e37e2f0f965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/extracted_features_round3.csv')\n",
    "training_set = df.drop(columns=['index', 'title', 'article', 'title_keywords', 'article_keywords', 'title_tokens_spacy', 'article_tokens_spacy', 'title_lemmas_spacy', 'article_lemmas_spacy', 'title_keywords', 'article_keywords', 'title_keyword_lemmas_spacy', 'article_sentences', 'article_sentence_tokens_spacy', 'title_tokens_no_stop_spacy', 'article_tokens_no_stop_spacy', 'article_sentence_tokens_no_stop_spacy'])\n",
    "training_set = training_set.rename(columns={'label':'CV'})\n",
    "training_set.to_csv('../../data/extracted_features_round3_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "00e28b32-e868-473c-9913-921d570c09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch#pytorch\n",
    "from transformers import AutoTokenizer, AutoModel#for embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity#for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "11c7ba2c-1583-4270-9bb4-483eece4f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",)\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "924184f3-96aa-49a9-b4fa-2a2164f43019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bert(text, token_length):\n",
    "    tokens=bert_tokenizer(text,max_length=token_length,padding='max_length',truncation=True)\n",
    "    output=bert_model(torch.tensor(tokens.input_ids).unsqueeze(0),\n",
    "               attention_mask=torch.tensor(tokens.attention_mask).unsqueeze(0)).hidden_states[-1]\n",
    "    return torch.mean(output,axis=1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f298fad5-ecc9-46c1-a77e-e1f4f703229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_vectors_bert'] = df['title'].apply(get_embeddings_bert, args=(256,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "beb75aa6-9ca2-4223-90e2-42ab5c7d1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_sentence_vectors_bert'] = df['article_sentences'].apply(lambda x: [get_embeddings_bert(str(sentence), 256) for sentence in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5d9ab0bb-5d6c-4d27-8c3e-51f962f05f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_sentence_similarity_score_bert(row):\n",
    "    title_vector = row['title_vectors_bert']\n",
    "    article_vectors = row['article_sentence_vectors_bert']\n",
    "        \n",
    "    scores = []\n",
    "    for vector in article_vectors:\n",
    "        scores.append(cosine_similarity(title_vector, vector)[0][0])\n",
    "    \n",
    "    return max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e2e20318-2b6a-46cf-b5b4-5c2a63fd6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TABSSIMBERT'] = df.apply(get_best_sentence_similarity_score_bert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "98c57f8f-1fe6-4fd4-ac73-a86f64bcf9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_sentence_similarity_score_bert(row):\n",
    "    title_vector = row['title_vectors_bert']\n",
    "    article_vectors = row['article_sentence_vectors_bert']\n",
    "        \n",
    "    scores = []\n",
    "    for vector in article_vectors:\n",
    "        scores.append(cosine_similarity(title_vector, vector)[0][0])\n",
    "    \n",
    "    return min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c7b118d9-39d4-4481-a390-c91cad195a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAWSSIMBERT'] = df.apply(get_worst_sentence_similarity_score_bert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2024089d-5b7d-4741-b50f-0e6021d55a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_sentence_similarity_score_bert(row):\n",
    "    title_vector = row['title_vectors_bert']\n",
    "    article_vectors = row['article_sentence_vectors_bert']\n",
    "        \n",
    "    score = 0\n",
    "    for vector in article_vectors:\n",
    "        score = score + cosine_similarity(title_vector, vector)[0][0]\n",
    "    \n",
    "    return score / len(article_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5cabf0c6-9ad2-42db-b04e-7f6d680c27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAASSIMSBERT'] = df.apply(get_average_sentence_similarity_score_bert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f30b0a9a-f820-4d61-9eaa-95770a5d3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_sentence_similarity_score_bert(row):\n",
    "    title_vector = row['title_vectors_bert']\n",
    "    article_vector = row['article_sentence_vectors_bert'][0]\n",
    "    \n",
    "    return cosine_similarity(title_vector, article_vector)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "88c6c4fe-4c62-4e3e-8fec-db21654c54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAFSSIMBERT'] = df.apply(get_first_sentence_similarity_score_bert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "eb6f58e6-f44b-4acf-b18a-936b1986b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_sentence_similarity_position_bert(row):\n",
    "    title_vector = row['title_vectors_bert']\n",
    "    article_vectors = row['article_sentence_vectors_bert']\n",
    "        \n",
    "    scores = []\n",
    "    for vector in article_vectors:\n",
    "        scores.append(cosine_similarity(title_vector, vector)[0][0])\n",
    "    \n",
    "    max_score = max(scores)\n",
    "    \n",
    "    return (scores.index(max_score) + 1) / len(article_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "63fa8fb1-cc17-4179-bf4e-7f66d10f38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TABSSIMPBERT'] = df.apply(get_best_sentence_similarity_position_bert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "769d2635-a15a-45af-a5a3-882fd8b862e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_sentence_similarity_position_bert(row):\n",
    "    title_vector = row['title_vectors_bert']\n",
    "    article_vectors = row['article_sentence_vectors_bert']\n",
    "    scores = []\n",
    "    for vector in article_vectors:\n",
    "        scores.append(cosine_similarity(title_vector, vector)[0][0])\n",
    "    \n",
    "    min_score = min(scores)\n",
    "    \n",
    "    return (scores.index(min_score) + 1) / len(article_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "e4202bea-4bcc-43b8-843a-bbd89571f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAWSSIMPBERT'] = df.apply(get_worst_sentence_similarity_position_bert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "63ce836a-4f4a-47f0-add0-94d8f8b45243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_sentence_similarity_ratio_bert(row):\n",
    "    title_keywords = row['title_keywords']\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "        \n",
    "    scores = []\n",
    "    for doc in article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "    \n",
    "    max_score = max(scores)\n",
    "    \n",
    "    return (scores.index(max_score) + 1) / len(article_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "eb22dbb2-3843-435a-8d2c-195fc97bf6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAGSSIMRATIOBERT'] = df.apply(get_good_sentence_similarity_ratio_bert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "2c1c882a-0e1e-45d0-afc4-af32b264b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_keyword_average_sentence_similarity_spacy(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    \n",
    "    if len(title_lemmas) == 0 or len(article_docs) == 0:\n",
    "        return 1\n",
    "    title_max_importance = min(number for word, number in title_lemmas)\n",
    "    title_max_lemma = next((word for word, number in title_lemmas if number == title_max_importance))\n",
    "    \n",
    "    matching_article_docs = []\n",
    "    for doc in article_docs:\n",
    "        if title_max_lemma in [token.lemma_ for token in doc]:\n",
    "            matching_article_docs.append(doc)\n",
    "    \n",
    "    score = 0\n",
    "    for doc in matching_article_docs:\n",
    "        score = score + (title_doc.similarity(doc))\n",
    "    \n",
    "    return score / len(article_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "494a0b48-8c10-4376-8598-bafa18121a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMIKWASSIMSPACY'] = df.apply(get_most_important_keyword_average_sentence_similarity_spacy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "93fb9772-c881-49e8-a316-54481bab7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_keyword_best_sentence_similarity_spacy(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    \n",
    "    if len(title_lemmas) == 0 or len(article_docs) == 0:\n",
    "        return 1\n",
    "    title_max_importance = min(number for word, number in title_lemmas)\n",
    "    title_max_lemma = next((word for word, number in title_lemmas if number == title_max_importance))\n",
    "    \n",
    "    matching_article_docs = []\n",
    "    for doc in article_docs:\n",
    "        if title_max_lemma in [token.lemma_ for token in doc]:\n",
    "            matching_article_docs.append(doc)\n",
    "    \n",
    "    scores = []\n",
    "    for doc in matching_article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "    \n",
    "    return max(scores) if len(scores) != 0 else 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "97a8bc78-c79e-4d1c-a0a0-a38b8d599985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMIKWBSSIMSPACY'] = df.apply(get_most_important_keyword_best_sentence_similarity_spacy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "8e5fb981-17a4-45df-9d55-f4a43478ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_keyword_worst_sentence_similarity_spacy(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    \n",
    "    if len(title_lemmas) == 0 or len(article_docs) == 0:\n",
    "        return 1\n",
    "    title_max_importance = min(number for word, number in title_lemmas)\n",
    "    title_max_lemma = next((word for word, number in title_lemmas if number == title_max_importance))\n",
    "    \n",
    "    matching_article_docs = []\n",
    "    for doc in article_docs:\n",
    "        if title_max_lemma in [token.lemma_ for token in doc]:\n",
    "            matching_article_docs.append(doc)\n",
    "    \n",
    "    scores = []\n",
    "    for doc in matching_article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "    \n",
    "    return min(scores) if len(scores) != 0 else 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "bfb69fef-6610-42de-887a-9c4239d4a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMIKWWSSIMSPACY'] = df.apply(get_most_important_keyword_worst_sentence_similarity_spacy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "9a5fd459-5a13-402f-8a9c-e6213ee3e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_keywords_similarity(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    \n",
    "    max_count = 0\n",
    "    sentences = []\n",
    "    for doc in article_docs:\n",
    "        count = 0\n",
    "        for title_lemma in title_lemmas:\n",
    "            if title_lemma in [token.lemma_ for token in doc]:\n",
    "                count = count + 1\n",
    "        if count > max_count:\n",
    "            sentences = [doc]\n",
    "        elif count == max_count:\n",
    "            sentences.append(doc)\n",
    "            \n",
    "    sim_score = 0\n",
    "    for sentence in sentences:\n",
    "        sim_score = sim_score + title_doc.similarity(sentence)\n",
    "    \n",
    "    return sim_score / len(sentences) if len(sentences) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "4d1c436d-5a73-44a1-9b24-4383b3dec2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp/ipykernel_32408/1740321506.py:20: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  sim_score = sim_score + title_doc.similarity(sentence)\n"
     ]
    }
   ],
   "source": [
    "df['TAMKWSSIMSPACY'] = df.apply(get_most_keywords_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "2076ee9f-04f2-4cad-a238-6bf6fdf9a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_keywords_sentence_polarity(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    article_sentences = row['article_sentences']\n",
    "    \n",
    "    max_count = 0\n",
    "    sentences = []\n",
    "    sentence_index = 0\n",
    "    for doc in article_docs:\n",
    "        count = 0\n",
    "        for title_lemma in title_lemmas:\n",
    "            if title_lemma in [token.lemma_ for token in doc]:\n",
    "                count = count + 1\n",
    "        if count > max_count:\n",
    "            sentences = [sentence_index]\n",
    "        elif count == max_count:\n",
    "            sentences.append(sentence_index)\n",
    "        sentence_index = sentence_index + 1\n",
    "            \n",
    "    pol_score = 0\n",
    "    for sentence in sentences:\n",
    "        pol_score = pol_score + TextBlob(str(article_sentences[sentence])).sentiment.polarity\n",
    "    \n",
    "    return pol_score / len(sentences) if len(sentences) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "c2982d5b-6f62-4cd8-9c48-1541f5fa35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMKWSP'] = df.apply(get_most_keywords_sentence_polarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "629e6358-86cd-4415-b410-d93482caa62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMKWSPD'] = df['TAMKWSP'] - df['TP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "fede427b-253e-4f08-9796-64bca7bb4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_keywords_sentence_subjectivity(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    article_sentences = row['article_sentences']\n",
    "    \n",
    "    max_count = 0\n",
    "    sentences = []\n",
    "    sentence_index = 0\n",
    "    for doc in article_docs:\n",
    "        count = 0\n",
    "        for title_lemma in title_lemmas:\n",
    "            if title_lemma in [token.lemma_ for token in doc]:\n",
    "                count = count + 1\n",
    "        if count > max_count:\n",
    "            sentences = [sentence_index]\n",
    "        elif count == max_count:\n",
    "            sentences.append(sentence_index)\n",
    "        sentence_index = sentence_index + 1\n",
    "            \n",
    "    pol_score = 0\n",
    "    for sentence in sentences:\n",
    "        pol_score = pol_score + TextBlob(str(article_sentences[sentence])).sentiment.subjectivity\n",
    "    \n",
    "    return pol_score / len(sentences) if len(sentences) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "67368f1c-cdf5-4dc9-b874-ce0e277042a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMKWSS'] = df.apply(get_most_keywords_sentence_subjectivity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "b2abcdec-eb1f-4288-b1a2-391c572b96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMKWSSD'] = df['TAMKWSS'] - df['TP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63881f53-677c-4ff3-a61f-c7b9b66a526b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea9998-3783-4517-8c8b-5613cddf3897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "5d1f5840-b560-482b-8bc5-31ac7d55529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_keywords_sentence_position(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    article_sentences = row['article_sentences']\n",
    "    \n",
    "    max_count = 0\n",
    "    best_sentence_index = 0\n",
    "    sentence_index = 1\n",
    "    for doc in article_docs:\n",
    "        count = 0\n",
    "        for title_lemma in title_lemmas:\n",
    "            if title_lemma in [token.lemma_ for token in doc]:\n",
    "                count = count + 1\n",
    "        if count > max_count:\n",
    "            best_sentence_index = sentence_index\n",
    "        sentence_index = sentence_index + 1\n",
    "            \n",
    "    return best_sentence_index / len(article_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "0549c6a9-3a52-4f81-a2a4-595155116374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAMKWFP'] = df.apply(get_most_keywords_sentence_position, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "c529a389-ac2e-4c65-bda4-1d98e143e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_sentences_similarity(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    \n",
    "    sentences = []\n",
    "    for doc in article_docs:\n",
    "        count = 0\n",
    "        for title_lemma in title_lemmas:\n",
    "            if title_lemma in [token.lemma_ for token in doc]:\n",
    "                count = count + 1\n",
    "        if count > 0:\n",
    "            sentences.append(doc)\n",
    "           \n",
    "            \n",
    "    sim_score = 0\n",
    "    for sentence in sentences:\n",
    "        sim_score = sim_score + title_doc.similarity(sentence)\n",
    "    \n",
    "    return sim_score / len(sentences) if len(sentences) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "b98bad57-80fc-4dd6-95e9-bc46fcd77ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWSIMSPACY'] = df.apply(get_keyword_sentences_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "fa3fcb89-331b-43af-8bc1-b503ec2b3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_sentences_polarity(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    article_sentences = row['article_sentences']\n",
    "    \n",
    "    max_count = 0\n",
    "    sentences = []\n",
    "    sentence_index = 0\n",
    "    for doc in article_docs:\n",
    "        count = 0\n",
    "        for title_lemma in title_lemmas:\n",
    "            if title_lemma in [token.lemma_ for token in doc]:\n",
    "                count = count + 1\n",
    "        if count > 1:\n",
    "            sentences.append(sentence_index)\n",
    "            \n",
    "    pol_score = 0\n",
    "    for sentence in sentences:\n",
    "        pol_score = pol_score + TextBlob(str(article_sentences[sentence])).sentiment.polarity\n",
    "    \n",
    "    return pol_score / len(sentences) if len(sentences) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "7192ef05-7580-4b54-a9ba-155245682908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWSP'] = df.apply(get_keyword_sentences_polarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "5635c978-6533-4ed8-a337-478240e8a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWSPD'] = df['TAKWSP'] - df['TP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "262dbbe6-30a1-42e1-9471-6d845af1db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_sentences_subjectivity(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    article_sentences = row['article_sentences']\n",
    "    \n",
    "    max_count = 0\n",
    "    sentences = []\n",
    "    sentence_index = 0\n",
    "    for doc in article_docs:\n",
    "        count = 0\n",
    "        for title_lemma in title_lemmas:\n",
    "            if title_lemma in [token.lemma_ for token in doc]:\n",
    "                count = count + 1\n",
    "        if count > 1:\n",
    "            sentences.append(sentence_index)\n",
    "            \n",
    "    pol_score = 0\n",
    "    for sentence in sentences:\n",
    "        pol_score = pol_score + TextBlob(str(article_sentences[sentence])).sentiment.subjectivity\n",
    "    \n",
    "    return pol_score / len(sentences) if len(sentences) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "2a82d025-02ae-473a-b21c-98bd0317346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWSS'] = df.apply(get_keyword_sentences_polarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "8173468e-9cbd-42d5-9b6a-15fad24fa38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWSSD'] = df['TAKWSS'] - df['TS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "79afe985-2deb-46c4-ac13-723c2922f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_sentences_similarity_variance(row):\n",
    "    title_lemmas = row['title_keyword_lemmas_spacy']\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    \n",
    "    sentences = []\n",
    "    for doc in article_docs:\n",
    "        count = 0\n",
    "        for title_lemma in title_lemmas:\n",
    "            if title_lemma in [token.lemma_ for token in doc]:\n",
    "                count = count + 1\n",
    "        if count > 0:\n",
    "            sentences.append(doc)\n",
    "           \n",
    "            \n",
    "    sim_scores = []\n",
    "    for sentence in sentences:\n",
    "        sim_score = sim_scores.append(title_doc.similarity(sentence))\n",
    "    \n",
    "    return numpy.var(sim_scores) / row['AL'] if len(sim_scores) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "868146f4-edeb-40df-af42-d54dc3e1a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAKWSIMVARSPACY'] = df.apply(get_keyword_sentences_similarity_variance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "f98946d0-d959-4219-9199-7ce9bee62898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_sentence_similarity_distribution(row):\n",
    "    title_doc = row['title_tokens_no_stop_spacy']\n",
    "    article_docs = row['article_sentence_tokens_no_stop_spacy']\n",
    "    best_sim = row['TABSSIMSPACY']\n",
    "        \n",
    "    scores = []\n",
    "    for doc in article_docs:\n",
    "        scores.append(title_doc.similarity(doc))\n",
    "        \n",
    "    condition = lambda x: x>=0.90\n",
    "    \n",
    "    index = 0\n",
    "    indexes = []\n",
    "    for score in scores:\n",
    "        if score >= (best_sim * 0.8):\n",
    "            indexes.append(index)\n",
    "        index = index + 1\n",
    "    \n",
    "    \n",
    "    distribution = 0\n",
    "    for i in indexes:\n",
    "        distribution = distribution + (i - (len(article_docs) - 1) / 2)\n",
    "            \n",
    "    return distribution / ((len(article_docs) - 1) / 2) if ((len(article_docs) - 1) / 2) != 0 else distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "0637945b-6d04-4b12-babc-c9fef602e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp/ipykernel_32408/1416477517.py:8: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  scores.append(title_doc.similarity(doc))\n"
     ]
    }
   ],
   "source": [
    "df['TAGSSIMDISTSPACY'] = df.apply(get_good_sentence_similarity_distribution, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bec5d-7f51-424e-a42d-05b18333b8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7edf40-259c-48b9-9df9-5308bded98f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0f6a2ceb-c5f0-4571-832a-92730a451acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/extracted_features_round4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c11739a8-a045-4c62-bf7c-4f33dd20a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_set = df.drop(columns=['index', 'title', 'article', 'title_keywords', 'article_keywords', 'title_tokens_spacy', 'article_tokens_spacy', 'title_lemmas_spacy', 'article_lemmas_spacy', 'title_keywords', 'article_keywords', 'title_keyword_lemmas_spacy', 'article_sentences', 'article_sentence_tokens_spacy', 'title_tokens_no_stop_spacy', 'article_tokens_no_stop_spacy', 'article_sentence_tokens_no_stop_spacy', 'title_vectors_bert', 'article_sentence_vectors_bert'])\n",
    "training_set = training_set.rename(columns={'label':'CV'})\n",
    "training_set.to_csv('../../data/extracted_features_round4_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "e2244a0a-3a1f-4798-80bb-7aaf238ae020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "      <th>TL</th>\n",
       "      <th>AL</th>\n",
       "      <th>title_tokens_spacy</th>\n",
       "      <th>article_tokens_spacy</th>\n",
       "      <th>title_lemmas_spacy</th>\n",
       "      <th>article_lemmas_spacy</th>\n",
       "      <th>...</th>\n",
       "      <th>TAWSSIMBERT</th>\n",
       "      <th>TAASSIMSBERT</th>\n",
       "      <th>TAFSSIMBERT</th>\n",
       "      <th>TABSSIMPBERT</th>\n",
       "      <th>TAWSSIMPBERT</th>\n",
       "      <th>TAGSSIMRATIOBERT</th>\n",
       "      <th>TAMIKWASSIMSPACY</th>\n",
       "      <th>TAMIKWBSSIMSPACY</th>\n",
       "      <th>TAMIKWWSSIMSPACY</th>\n",
       "      <th>title_keyword_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1305</td>\n",
       "      <td>Nikki Bella Now Believes John Cena Wants Kids,...</td>\n",
       "      <td>John Cena's very public campaign to win back N...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>68</td>\n",
       "      <td>1537</td>\n",
       "      <td>(Nikki, Bella, Now, Believes, John, Cena, Want...</td>\n",
       "      <td>(John, Cena, 's, very, public, campaign, to, w...</td>\n",
       "      <td>[Nikki, Bella, now, believe, John, Cena, want,...</td>\n",
       "      <td>[John, Cena, 's, very, public, campaign, to, w...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646927</td>\n",
       "      <td>0.751304</td>\n",
       "      <td>0.855155</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.085057</td>\n",
       "      <td>0.751116</td>\n",
       "      <td>0.609801</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6528</td>\n",
       "      <td>Rouhani says Iran will keep producing missiles...</td>\n",
       "      <td>DUBAI (Reuters) - Iran will continue to produc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64</td>\n",
       "      <td>3753</td>\n",
       "      <td>(Rouhani, says, Iran, will, keep, producing, m...</td>\n",
       "      <td>(DUBAI, (, Reuters, ), -, Iran, will, continue...</td>\n",
       "      <td>[Rouhani, say, Iran, will, keep, produce, miss...</td>\n",
       "      <td>[DUBAI, (, Reuters, ), -, Iran, will, continue...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606608</td>\n",
       "      <td>0.817025</td>\n",
       "      <td>0.876656</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.465548</td>\n",
       "      <td>0.868273</td>\n",
       "      <td>0.749374</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13823</td>\n",
       "      <td>Jerry Jones: Ezekiel Elliott Won't Be Suspended</td>\n",
       "      <td>Jerry Jones tells TMZ Sports he does not belie...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>47</td>\n",
       "      <td>686</td>\n",
       "      <td>(Jerry, Jones, :, Ezekiel, Elliott, Wo, n't, B...</td>\n",
       "      <td>(Jerry, Jones, tells, TMZ, Sports, he, does, n...</td>\n",
       "      <td>[Jerry, Jones, :, Ezekiel, Elliott, will, not,...</td>\n",
       "      <td>[Jerry, Jones, tell, TMZ, Sports, he, do, not,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.732938</td>\n",
       "      <td>0.777570</td>\n",
       "      <td>0.871304</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.260440</td>\n",
       "      <td>0.743468</td>\n",
       "      <td>0.521139</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29908</td>\n",
       "      <td>Sen. Marco Rubio just threatened to take care...</td>\n",
       "      <td>The decorum of the United States Senate was on...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>73</td>\n",
       "      <td>2788</td>\n",
       "      <td>(Sen., Marco, Rubio, just, threatened, to, , ...</td>\n",
       "      <td>(The, decorum, of, the, United, States, Senate...</td>\n",
       "      <td>[Sen., Marco, Rubio, just, threaten, to, \", ta...</td>\n",
       "      <td>[the, decorum, of, the, United, States, Senate...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565238</td>\n",
       "      <td>0.800380</td>\n",
       "      <td>0.866890</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31325</td>\n",
       "      <td>Darius McCrary Says Estranged Wife's Out for F...</td>\n",
       "      <td>Former \"Family Matters\" star Darius McCrary to...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>67</td>\n",
       "      <td>730</td>\n",
       "      <td>(Darius, McCrary, Says, Estranged, Wife, 's, O...</td>\n",
       "      <td>(Former, \", Family, Matters, \", star, Darius, ...</td>\n",
       "      <td>[Darius, McCrary, say, estranged, Wife, 's, ou...</td>\n",
       "      <td>[former, \", Family, matter, \", star, Darius, M...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803187</td>\n",
       "      <td>0.832320</td>\n",
       "      <td>0.879853</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.253621</td>\n",
       "      <td>0.713680</td>\n",
       "      <td>0.554424</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>2678751</td>\n",
       "      <td>Stimulus bill: When will you see money and ben...</td>\n",
       "      <td>(CNN)Congress and President Donald Trump have...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>52</td>\n",
       "      <td>7393</td>\n",
       "      <td>(Stimulus, bill, :, When, will, you, see, mone...</td>\n",
       "      <td>( , (, CNN)Congress, and, President, Donald, T...</td>\n",
       "      <td>[stimulus, bill, :, when, will, you, see, mone...</td>\n",
       "      <td>[ , (, cnn)congress, and, President, Donald, T...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734519</td>\n",
       "      <td>0.790901</td>\n",
       "      <td>0.792949</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.049837</td>\n",
       "      <td>0.874256</td>\n",
       "      <td>0.758666</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>2679327</td>\n",
       "      <td>California coronavirus: Doctors 'cautiously ho...</td>\n",
       "      <td>(CNN)Two weeks after San Francisco issued the...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>95</td>\n",
       "      <td>3193</td>\n",
       "      <td>(California, coronavirus, :, Doctors, ', cauti...</td>\n",
       "      <td>( , (, CNN)Two, weeks, after, San, Francisco, ...</td>\n",
       "      <td>[California, coronavirus, :, doctor, ', cautio...</td>\n",
       "      <td>[ , (, cnn)two, week, after, San, Francisco, i...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669416</td>\n",
       "      <td>0.787754</td>\n",
       "      <td>0.856837</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.031917</td>\n",
       "      <td>0.797926</td>\n",
       "      <td>0.797926</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>2683873</td>\n",
       "      <td>$4.3 Million Homes in California</td>\n",
       "      <td>What you Get A French Caribbean-inspired home ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32</td>\n",
       "      <td>8775</td>\n",
       "      <td>($, 4.3, Million, Homes, in, California)</td>\n",
       "      <td>(What, you, Get, A, French, Caribbean, -, insp...</td>\n",
       "      <td>[$, 4.3, million, Homes, in, California]</td>\n",
       "      <td>[what, you, get, a, french, Caribbean, -, insp...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515683</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.723886</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>2684134</td>\n",
       "      <td>Ventilators and Coronavirus: Amid Desperation,...</td>\n",
       "      <td>President Trump wants the private sector to fi...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>82</td>\n",
       "      <td>9977</td>\n",
       "      <td>(Ventilators, and, Coronavirus, :, Amid, Despe...</td>\n",
       "      <td>(President, Trump, wants, the, private, sector...</td>\n",
       "      <td>[ventilator, and, Coronavirus, :, amid, desper...</td>\n",
       "      <td>[President, Trump, want, the, private, sector,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542087</td>\n",
       "      <td>0.779070</td>\n",
       "      <td>0.821221</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.032506</td>\n",
       "      <td>0.781294</td>\n",
       "      <td>0.741802</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>2688615</td>\n",
       "      <td>How to choose a free videoconference app</td>\n",
       "      <td>Most people are currently relying on videoconf...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>40</td>\n",
       "      <td>5145</td>\n",
       "      <td>(How, to, choose, a, free, videoconference, app)</td>\n",
       "      <td>(Most, people, are, currently, relying, on, vi...</td>\n",
       "      <td>[how, to, choose, a, free, videoconference, app]</td>\n",
       "      <td>[Most, people, be, currently, rely, on, videoc...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.762374</td>\n",
       "      <td>0.766763</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.229402</td>\n",
       "      <td>0.838025</td>\n",
       "      <td>0.648460</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>967 rows  91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                              title  \\\n",
       "0       1305  Nikki Bella Now Believes John Cena Wants Kids,...   \n",
       "1       6528  Rouhani says Iran will keep producing missiles...   \n",
       "2      13823    Jerry Jones: Ezekiel Elliott Won't Be Suspended   \n",
       "3      29908  Sen. Marco Rubio just threatened to take care...   \n",
       "4      31325  Darius McCrary Says Estranged Wife's Out for F...   \n",
       "..       ...                                                ...   \n",
       "962  2678751  Stimulus bill: When will you see money and ben...   \n",
       "963  2679327  California coronavirus: Doctors 'cautiously ho...   \n",
       "964  2683873                   $4.3 Million Homes in California   \n",
       "965  2684134  Ventilators and Coronavirus: Amid Desperation,...   \n",
       "966  2688615           How to choose a free videoconference app   \n",
       "\n",
       "                                               article  label  TL    AL  \\\n",
       "0    John Cena's very public campaign to win back N...    4.5  68  1537   \n",
       "1    DUBAI (Reuters) - Iran will continue to produc...    3.0  64  3753   \n",
       "2    Jerry Jones tells TMZ Sports he does not belie...    4.5  47   686   \n",
       "3    The decorum of the United States Senate was on...    3.5  73  2788   \n",
       "4    Former \"Family Matters\" star Darius McCrary to...    3.0  67   730   \n",
       "..                                                 ...    ...  ..   ...   \n",
       "962   (CNN)Congress and President Donald Trump have...    3.5  52  7393   \n",
       "963   (CNN)Two weeks after San Francisco issued the...    4.0  95  3193   \n",
       "964  What you Get A French Caribbean-inspired home ...    4.0  32  8775   \n",
       "965  President Trump wants the private sector to fi...    2.5  82  9977   \n",
       "966  Most people are currently relying on videoconf...    4.5  40  5145   \n",
       "\n",
       "                                    title_tokens_spacy  \\\n",
       "0    (Nikki, Bella, Now, Believes, John, Cena, Want...   \n",
       "1    (Rouhani, says, Iran, will, keep, producing, m...   \n",
       "2    (Jerry, Jones, :, Ezekiel, Elliott, Wo, n't, B...   \n",
       "3    (Sen., Marco, Rubio, just, threatened, to, , ...   \n",
       "4    (Darius, McCrary, Says, Estranged, Wife, 's, O...   \n",
       "..                                                 ...   \n",
       "962  (Stimulus, bill, :, When, will, you, see, mone...   \n",
       "963  (California, coronavirus, :, Doctors, ', cauti...   \n",
       "964           ($, 4.3, Million, Homes, in, California)   \n",
       "965  (Ventilators, and, Coronavirus, :, Amid, Despe...   \n",
       "966   (How, to, choose, a, free, videoconference, app)   \n",
       "\n",
       "                                  article_tokens_spacy  \\\n",
       "0    (John, Cena, 's, very, public, campaign, to, w...   \n",
       "1    (DUBAI, (, Reuters, ), -, Iran, will, continue...   \n",
       "2    (Jerry, Jones, tells, TMZ, Sports, he, does, n...   \n",
       "3    (The, decorum, of, the, United, States, Senate...   \n",
       "4    (Former, \", Family, Matters, \", star, Darius, ...   \n",
       "..                                                 ...   \n",
       "962  ( , (, CNN)Congress, and, President, Donald, T...   \n",
       "963  ( , (, CNN)Two, weeks, after, San, Francisco, ...   \n",
       "964  (What, you, Get, A, French, Caribbean, -, insp...   \n",
       "965  (President, Trump, wants, the, private, sector...   \n",
       "966  (Most, people, are, currently, relying, on, vi...   \n",
       "\n",
       "                                    title_lemmas_spacy  \\\n",
       "0    [Nikki, Bella, now, believe, John, Cena, want,...   \n",
       "1    [Rouhani, say, Iran, will, keep, produce, miss...   \n",
       "2    [Jerry, Jones, :, Ezekiel, Elliott, will, not,...   \n",
       "3    [Sen., Marco, Rubio, just, threaten, to, \", ta...   \n",
       "4    [Darius, McCrary, say, estranged, Wife, 's, ou...   \n",
       "..                                                 ...   \n",
       "962  [stimulus, bill, :, when, will, you, see, mone...   \n",
       "963  [California, coronavirus, :, doctor, ', cautio...   \n",
       "964           [$, 4.3, million, Homes, in, California]   \n",
       "965  [ventilator, and, Coronavirus, :, amid, desper...   \n",
       "966   [how, to, choose, a, free, videoconference, app]   \n",
       "\n",
       "                                  article_lemmas_spacy  ... TAWSSIMBERT  \\\n",
       "0    [John, Cena, 's, very, public, campaign, to, w...  ...    0.646927   \n",
       "1    [DUBAI, (, Reuters, ), -, Iran, will, continue...  ...    0.606608   \n",
       "2    [Jerry, Jones, tell, TMZ, Sports, he, do, not,...  ...    0.732938   \n",
       "3    [the, decorum, of, the, United, States, Senate...  ...    0.565238   \n",
       "4    [former, \", Family, matter, \", star, Darius, M...  ...    0.803187   \n",
       "..                                                 ...  ...         ...   \n",
       "962  [ , (, cnn)congress, and, President, Donald, T...  ...    0.734519   \n",
       "963  [ , (, cnn)two, week, after, San, Francisco, i...  ...    0.669416   \n",
       "964  [what, you, get, a, french, Caribbean, -, insp...  ...    0.515683   \n",
       "965  [President, Trump, want, the, private, sector,...  ...    0.542087   \n",
       "966  [Most, people, be, currently, rely, on, videoc...  ...    0.689500   \n",
       "\n",
       "    TAASSIMSBERT  TAFSSIMBERT  TABSSIMPBERT  TAWSSIMPBERT  TAGSSIMRATIOBERT  \\\n",
       "0       0.751304     0.855155      0.062500      0.812500          0.250000   \n",
       "1       0.817025     0.876656      0.052632      0.736842          0.789474   \n",
       "2       0.777570     0.871304      0.142857      0.428571          0.285714   \n",
       "3       0.800380     0.866890      0.090909      0.954545          0.545455   \n",
       "4       0.832320     0.879853      0.200000      0.800000          1.000000   \n",
       "..           ...          ...           ...           ...               ...   \n",
       "962     0.790901     0.792949      0.062500      0.265625          0.406250   \n",
       "963     0.787754     0.856837      0.040000      0.800000          0.480000   \n",
       "964     0.607595     0.723886      0.013889      0.833333          0.000000   \n",
       "965     0.779070     0.821221      0.100000      1.000000          0.414286   \n",
       "966     0.762374     0.766763      0.757576      0.909091          0.060606   \n",
       "\n",
       "     TAMIKWASSIMSPACY  TAMIKWBSSIMSPACY  TAMIKWWSSIMSPACY  title_keyword_count  \n",
       "0            0.085057          0.751116          0.609801                    6  \n",
       "1            0.465548          0.868273          0.749374                    6  \n",
       "2            0.260440          0.743468          0.521139                    5  \n",
       "3            0.000000          0.800000          0.700000                    8  \n",
       "4            0.253621          0.713680          0.554424                    7  \n",
       "..                ...               ...               ...                  ...  \n",
       "962          0.049837          0.874256          0.758666                    4  \n",
       "963          0.031917          0.797926          0.797926                   10  \n",
       "964          0.000000          0.800000          0.700000                    3  \n",
       "965          0.032506          0.781294          0.741802                    8  \n",
       "966          0.229402          0.838025          0.648460                    4  \n",
       "\n",
       "[967 rows x 91 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2505be1-db63-4402-8ef1-44b1be7ad114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
